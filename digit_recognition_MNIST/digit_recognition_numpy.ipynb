{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"generative_ai_disabled":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Mount Google Drive storage and import the data"],"metadata":{"id":"zwIile67G6Kj"}},{"cell_type":"code","source":["# Mount Google Drive storage\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Import training data\n","# First row = labels as strings, every other row = (answer,pixel data array)\n","data_train = pd.read_csv( \\\n","                \"/content/drive/MyDrive/google_colaboratory/data/train.csv\", \\\n","                header=0,dtype=float).values\n","\n","# Import test data\n","data_test = pd.read_csv( \\\n","                \"/content/drive/MyDrive/google_colaboratory/data/test.csv\", \\\n","                header=0,dtype=float).values\n"],"metadata":{"id":"v9F0l7P7rtIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Single-layer (softmax) and multi-layer (N*ReLU+softmax) perceptrons\n"],"metadata":{"id":"BaF8zEYoHDD7"}},{"cell_type":"markdown","source":["### Display an image from the training data"],"metadata":{"id":"eEeDUJAqIxo4"}},{"cell_type":"code","source":["# Pick an image\n","image_num = 16\n","label = data_train[image_num][0]\n","image = data_train[image_num][1:].reshape(28,28)\n","\n","# Display the image and its label\n","print(\"This number is: \",int(label))\n","plt.imshow(image, cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()\n"],"metadata":{"id":"xFpLS3TBriMx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define input/output and functions"],"metadata":{"id":"adWH-9ZvI7DH"}},{"cell_type":"code","source":["# DEFINE MODEL'S PARAMETERS, INPUTS AND OUTPUTS\n","\n","# Inputs:\n","# X = array(num_samples,num_features)\n","# (for each sample, pixel values in range [0,1], flattened 28x28-pixel image)\n","X = np.array(data_train[:,1:]/255.0)\n","# Mean subtraction: subtract average across dataset, to make features balanced\n","X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0)+10**(-9))\n","\n","# Outputs:\n","# Y = array(num_samples)\n","# (for each sample, correct class = digit)\n","Y = np.array(data_train[:,0].astype(int))\n","\n","# Number of classes for classification (digits -> 10 possible classes)\n","num_classes = 10\n","\n","# Number of samples (data points)\n","num_samples = X.shape[0]\n","\n","# Number of features (pixels in a single sample)\n","num_features = X.shape[1]\n","\n","# Number of hidden layers\n","num_layers = 3\n","\n","# Dimension of a hidden layer\n","num_hidden = 256\n","\n","# Generate array of initial parameters\n","def generate_parameter(dim_1, dim_2):\n","    W = np.sqrt(2.0/dim_1)*np.random.randn(dim_1,dim_2)\n","    b = np.full((1, dim_2), 0.01)\n","    return [W, b]\n","if num_layers==1: parameters_init = [generate_parameter(num_features,num_classes)]\n","else:\n","    parameters_init = [generate_parameter(num_features,num_hidden)]\n","    for layer in range(2,num_layers):\n","        parameters_init.append(generate_parameter(num_hidden,num_hidden))\n","    parameters_init.append(generate_parameter(num_hidden,num_classes))\n","\n","#-------------------------------------------------------------------------------\n","\n","# DEFINE FUNCTIONS\n","\n","# ReLU activation function (used for hidden layers): removes negative scores\n","def relu(Z): return np.maximum(0,Z)\n","def relu_derivative(Z): return (Z>0).astype(float)\n","\n","# SOFTMAX activation function (used for output layer):\n","# transforms arbitrary-valued scores (logits) into probabilities\n","# Z = array(num_samples,num_classes)\n","# (for each sample, arbitrary-valued scores for each class)\n","# softmax(Z) = array(num_samples,num_classes)\n","# (for each sample, probability of belonging to a certain class)\n","def softmax(Z):\n","    exp_z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n","    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","\n","# Function that converts outputs (labels) into corresponding probability vectors\n","# Y = array(num_samples)\n","# one_hot(Y) = array(num_samples,num_classes)\n","# (for each sample, probability of belonging to a certain class)\n","def one_hot(Y): return np.eye(num_classes)[Y]\n","\n","# LOSS FUNCTION - cross-entropy: calculates the punishment for a bad prediction\n","# = LOG(cummulative probability of predicting all outputs)\n","def cross_entropy(probs_pred, probs_true):\n","    # 10**(-9) is needed to avoid LOG(0) error\n","    return -np.sum(probs_true * np.log(probs_pred + 10**(-9))) / num_samples\n","def cross_entropy_derivative(probs_pred, probs_true):\n","    return (probs_pred - probs_true) / num_samples\n","\n","# FORWARD PROPAGATION\n","\n","# Single-layer forward propagation\n","def forward_SL(X, W, b, activation_function):\n","    # Calculate arbitrary-valued scores\n","    Z = X.dot(W) + b\n","    # Apply the activation function\n","    Y = activation_function(Z)\n","    return [Z, Y]\n","\n","# Multi-layer forward propagation\n","# Examples:\n","# params_array = [[W1,b1],[W2,b2],...]\n","def forward_ML(X_init, parameters):\n","    forward_parameters, X = [[0, X_init]], X_init\n","    # For every layer: calculate forward propagation and update initial vector\n","    for index, parameter in enumerate(parameters):\n","        # softmax is only applied at the outer level\n","        activation_function = relu if index!=(num_layers-1) else softmax\n","        forward_parameter = forward_SL(X,*parameter,activation_function)\n","        X = forward_parameter[1]\n","        forward_parameters.append(forward_parameter)\n","    # Export list of [Z, Y] for all layers:\n","    # index 0 corresponds to the initial data, so len(forward_params)=num_layers+1\n","    return forward_parameters\n","\n","# BACKWARD PROPAGATION\n","\n","# Single-layer backward propagation\n","# Examples:\n","# params = [cross_entropy_derivative, probs_true]\n","# params = [relu_derivative, W_next, dL_dZ_next]\n","def backward_SL(X, Z, derivative_parameters):\n","    function_derivative = derivative_parameters[0]\n","    # Cross-entropy is always the outer layer: probs_pred = Z\n","    if function_derivative.__name__=='cross_entropy_derivative':\n","        probs_pred, probs_true = softmax(Z), derivative_parameters[1]\n","        dL_dZ = cross_entropy_derivative(probs_pred, probs_true)\n","    # ReLU is always an inner layer, so it needs:\n","    # W_next and dL_dZ_next from the \"next\" (in terms of forward propagation) step\n","    elif function_derivative.__name__=='relu_derivative':\n","        W_next, dL_dZ_next = derivative_parameters[1:]\n","        # Current step's Y is \"next\" step's X\n","        dL_dY = dL_dZ_next.dot(W_next.T)\n","        dL_dZ = dL_dY * relu_derivative(Z)\n","    else:\n","        print(\"Wrong derivative function!\")\n","        exit()\n","    # This works for every layer: Z = W*X + b\n","    dL_dW = (X.T).dot(dL_dZ)\n","    dL_db = np.sum(dL_dZ, axis=0, keepdims=True)\n","    # Export derivatives\n","    return [dL_dZ, dL_dW, dL_db]\n","\n","# Multi-layer backward propagation\n","def backward_ML(X_init, probs_true, parameters, forward_parameters):\n","    derivatives = []\n","    # Layers are counted in the direction of forward propagation\n","    # layer 0 is the initial data\n","    for layer in range(num_layers-1,-1,-1):\n","        # If it is the outer layer, apply cross_entroy_derivative\n","        # If it is an inner layer, apply relu_derivative with:\n","        # W_next = parameters[layer-1,1]\n","        # dL_dZ_next = derivatives[0,0]\n","        backward_parameters = \\\n","            [cross_entropy_derivative, probs_true] if layer==num_layers-1 else \\\n","            [relu_derivative, parameters[layer+1][0], derivatives[0][0]]\n","        X, Z = forward_parameters[layer][1], forward_parameters[layer+1][0]\n","        derivative = backward_SL(X, Z, backward_parameters)\n","        derivatives.insert(0,derivative)\n","    return derivatives\n","\n","\n","# Training function\n","def train(X, Y, parameters = None, \\\n","          rate_init=0.05, num_epochs=300, lambda_reg=0.0001, momentum=0.9):\n","\n","    # Assign default parameters\n","    if parameters is None:\n","        parameters = [[W.copy(), b.copy()] for (W,b) in parameters_init]\n","\n","    # Probabilities of outputs\n","    probs_true = one_hot(Y)\n","\n","    # Initialize velocities\n","    # velocities = [[v_W1,v_b1],[v_W2,v_b2],...]\n","    velocities = []\n","    for parameter in parameters:\n","        velocities.append([np.zeros_like(parameter[0]),np.zeros_like(parameter[1])])\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","\n","        # Forward propagation\n","        forward_parameters = forward_ML(X, parameters)\n","\n","        # Backward propagation\n","        derivatives = backward_ML(X, probs_true, parameters, forward_parameters)\n","\n","        # Iterate over layers\n","        for layer in range(0,num_layers):\n","            # L2-regularization\n","            derivatives[layer][1] += lambda_reg*parameters[layer][0]\n","            # Re-calculate velocities\n","            velocities[layer][0] = momentum*velocities[layer][0] - rate_init*derivatives[layer][1]\n","            velocities[layer][1] = momentum*velocities[layer][1] - rate_init*derivatives[layer][2]\n","            # Perform gradient descent\n","            parameters[layer][0] += velocities[layer][0]\n","            parameters[layer][1] += velocities[layer][1]\n","\n","        # Print out loss function values\n","        if epoch % 10 == 0:\n","            # Calculate loss\n","            loss = cross_entropy(forward_parameters[-1][1],probs_true)\n","            # Calculate accuracy\n","            accuracy = np.mean(np.argmax(forward_parameters[-1][1], axis=1)==Y)\n","            print(f\"Epoch {epoch}, loss {loss:.4f}, accuracy {accuracy:.4f}\")\n","\n","    return parameters\n"],"metadata":{"id":"QVMga25Nxlli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters_0 = train(X, Y)"],"metadata":{"id":"AeSQee7frTK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(parameters_init)"],"metadata":{"id":"YO3GFcgd9guW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Forward propagation: ReLU+softmax\n","# H = output of hidden layer (score)\n","def forward2(X, W1, b1, W2, b2):\n","    # First layer: ReLU\n","    Z1 = X.dot(W1) + b1\n","    H = relu(Z1)\n","    # Second layer: softmax\n","    Z2 = H.dot(W2) + b2\n","    probs_pred = softmax(Z2)\n","    return Z1, H, Z2, probs_pred\n","\n","# Backward propagation\n","def backward2(X, Z1, H, W2, probs_pred, probs_true):\n","    # Gradient for softmax layer\n","    # d(loss)/d(z2)\n","    dL_dZ2 = cross_entropy_derivative(probs_pred, probs_true)\n","    # d(loss)/d(W2)\n","    dL_dW2 = (H.T).dot(dL_dZ2)\n","    dL_db2 = np.sum(dL_dZ2, axis=0, keepdims=True)\n","    # Gradient for ReLU layer\n","    dL_dH = dL_dZ2.dot(W2.T)\n","    dL_dZ1 = dL_dH * relu_derivative(Z1)\n","    dL_dW1 = (X.T).dot(dL_dZ1)\n","    dL_db1 = np.sum(dL_dZ1, axis=0, keepdims=True)\n","    return dL_dW2, dL_db2, dL_dW1, dL_db1\n","\n","# Training function: ReLU+softmax\n","def train2(X, Y, rate_init=0.05, num_epochs=300, lambda_reg=0.0001, momentum=0.9, \\\n","            W1 = np.sqrt(2.0/num_features)*np.random.randn(num_features,num_hidden), \\\n","            b1 = np.full((1, num_hidden), 0.01), \\\n","            W2 = np.sqrt(2.0/num_hidden)*np.random.randn(num_hidden,num_classes), \\\n","            b2 = np.full((1, num_classes), 0.01)):\n","\n","    # Initialize velocity terms\n","    v_W1, v_b1 = np.zeros_like(W1), np.zeros_like(b1)\n","    v_W2, v_b2 = np.zeros_like(W2), np.zeros_like(b2)\n","\n","    # Probabilities of outputs\n","    probs_true = one_hot(Y)\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","\n","        # Forward propagation\n","        Z1, H, Z2, probs_pred = forward2(X, W1, b1, W2, b2)\n","\n","        # Backward propagation\n","        dL_dW2, dL_db2, dL_dW1, dL_db1 = backward2(X,Z1,H,W2,probs_pred,probs_true)\n","\n","        # L2 regularization\n","        dL_dW1 += lambda_reg*W1\n","        dL_dW2 += lambda_reg*W2\n","\n","        # Update momenta\n","        v_W1 = momentum*v_W1 - rate_init*dL_dW1\n","        v_b1 = momentum*v_b1 - rate_init*dL_db1\n","        v_W2 = momentum*v_W2 - rate_init*dL_dW2\n","        v_b2 = momentum*v_b2 - rate_init*dL_db2\n","\n","        # Gradient descent\n","        W1 += v_W1\n","        b1 += v_b1\n","        W2 += v_W2\n","        b2 += v_b2\n","\n","        # Print out loss function values\n","        if epoch % 10 == 0:\n","            # Calculate loss\n","            loss = cross_entropy(probs_pred,probs_true)\n","            # Calculate accuracy\n","            accuracy = np.mean(np.argmax(probs_pred, axis=1)==Y)\n","            print(f\"Epoch {epoch}, loss {loss:.4f}, accuracy {accuracy:.4f}\")\n","\n","    return W1, b1, W2, b2"],"metadata":{"id":"xK9FpUIYoLCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W1_0, b1_0, W2_0, b2_0 = train2(X,Y)"],"metadata":{"id":"jbBtCeiaSc01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Inputs:\\n\",X.shape)\n","print(\"Outputs:\\n\",Y.shape,\"\\n\")\n","\n","scores = [[2,1,0],[2,3,5]]\n","print(\"Arbitrary valued scores:\\n\",scores)\n","print(\"Corresponding probabilities:\\n\",softmax(scores),\"\\n\")\n","\n","Y_true = [1,3]\n","Y_pred = [[1,2,4,5,0,0,0,1,2,1],[1,0,4,5,0,0,0,1,2,1]]\n","print(\"Model predictions (probabilities):\\n\",softmax(Y_pred))\n","print(\"Outputs (probabilities):\\n\",one_hot(Y_true))\n","print(\"Cross-entropy:\\n\",cross_entropy(one_hot(Y_true),softmax(Y_pred)))"],"metadata":{"id":"2t5u6iIp_hYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate predicted values.\n","# Outputs are classes with largest probabilities.\n","def predict(X, W1, b1, W2, b2):\n","    Z1 = X.dot(W1) + b1\n","    H = relu(Z1)\n","    Z2 = H.dot(W2) + b2\n","    probs_pred = softmax(Z2)\n","    return np.argmax(probs_pred, axis=1)\n","\n","Y_pred = predict(X, W1_0, b1_0, W2_0, b2_0)\n","\n","accuracy = np.mean(Y_pred == Y)\n","print(\"Training accuracy:\", accuracy)"],"metadata":{"id":"eN2jneLlUTs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate test values.\n","# Inputs:\n","# X = array(num_samples,num_features)\n","# (for each sample, pixel values in range [0,1], flattened 28x28-pixel image)\n","X_test = data_test[:,:]/255.0\n","# Mean subtraction: subtract average across dataset, to make features balanced\n","X_test = (X_test - np.mean(X_test, axis=0)) / (np.std(X_test, axis=0)+10**(-9))\n","\n","# Outputs:\n","# Y = array(num_samples)\n","# (for each sample, correct class = digit)\n","Y_test = data_test[:,0].astype(int)\n","\n","\n","# Outputs are classes with largest probabilities.\n","def predict(X, W1, b1, W2, b2):\n","    Z1 = X.dot(W1) + b1\n","    H = relu(Z1)\n","    Z2 = H.dot(W2) + b2\n","    probs_pred = softmax(Z2)\n","    return np.argmax(probs_pred, axis=1)\n","\n","Y_pred_test = predict(X_test, W1_0, b1_0, W2_0, b2_0)\n","\n","#accuracy = np.mean(Y_pred == Y_test)\n","#print(\"Testing accuracy:\", accuracy)"],"metadata":{"id":"MZOYxzroenpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking test data\n","\n","# Pick an image\n","image_num = 3\n","label = Y_pred_test[image_num]\n","image = data_test[image_num][:].reshape(28,28)\n","\n","# Display the image and its label\n","print(\"Predicted number is: \",int(label))\n","plt.imshow(image, cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"q3o9hNUPVBce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["matrix = (W1_0.dot(W2_0))[:,8].reshape(28,28)\n","plt.imshow(matrix, cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"dZzPjTE3gEWe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training test"],"metadata":{"id":"qgDLig9x8KPf"}},{"cell_type":"code","source":["# This function performs model training.\n","# W and b are model parameters\n","def train(X, Y, rate=0.1, num_epochs=300, \\\n","            W1 = 0.01*np.random.randn(num_features,num_hidden), \\\n","            b1 = np.zeros((1,num_hidden)), \\\n","            W2 = 0.01*np.random.randn(num_hidden,num_classes), \\\n","            b2 = np.zeros((1,num_classes))):\n","\n","    # Probabilities of outputs\n","    probs_true = one_hot(Y)\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","\n","        # Forward calculation:\n","\n","        # First layer: ReLU\n","\n","\n","\n","\n","\n","        # Calculate arbitrary-valued scores (logits)\n","        logits = X.dot(W) + b\n","        # Convert logits into probabilities\n","        probs_pred = softmax(logits)\n","\n","        # Calculate loss function\n","        loss = cross_entropy(probs_true, probs_pred)\n","\n","        # Calculate gradients of loss function wrt model parameters\n","        # d(loss)/d(logits)\n","        gradients = (probs_pred - probs_true) / num_samples\n","        # d(loss)/dW\n","        dW = (X.T).dot(gradients)\n","        # d(loss)/db\n","        db = np.sum(gradients, axis=0, keepdims=True)\n","\n","        # Update model parameters (gradient descent)\n","        W -= rate * dW\n","        b -= rate * db\n","\n","        # Print out loss function values\n","        if epoch % 5 == 0: print(f\"Epoch {epoch}, loss {loss:.4f}\")\n","\n","    # Return model parameters\n","    return W, b\n","\n","# Train model\n","# Initial run:\n","W0, b0 = train(X,Y)\n","# Additional runs:\n","W0,b0 = train(X,Y,rate=0.01,num_epochs=1000,W=W0,b=b0)"],"metadata":{"id":"oJcd6ugP8LFN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test functions"],"metadata":{"id":"57Rzyfy-Pjw9"}},{"cell_type":"code","source":["print(\"Inputs:\\n\",X.shape)\n","print(\"Outputs:\\n\",Y.shape,\"\\n\")\n","\n","scores = [[2,1,0],[2,3,5]]\n","print(\"Arbitrary valued scores:\\n\",scores)\n","print(\"Corresponding probabilities:\\n\",softmax(scores),\"\\n\")\n","\n","Y_true = [1,3]\n","Y_pred = [[1,2,4,5,0,0,0,1,2,1],[1,0,4,5,0,0,0,1,2,1]]\n","print(\"Model predictions (probabilities):\\n\",softmax(Y_pred))\n","print(\"Outputs (probabilities):\\n\",one_hot(Y_true))\n","print(\"Cross-entropy:\\n\",cross_entropy(one_hot(Y_true),softmax(Y_pred)))\n"],"metadata":{"id":"baRgGnckPQln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"rSDExNdmTPdo"}},{"cell_type":"code","source":["# This function performs model training.\n","# W and b are model parameters\n","def train(X, Y, rate=0.1, num_epochs=300, \\\n","            W = np.random.randn(num_features,num_classes)*0.01, \\\n","            b = np.zeros((1,num_classes))):\n","\n","    # Probabilities of outputs\n","    probs_true = one_hot(Y)\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","\n","        # Forward calculation:\n","        # Calculate arbitrary-valued scores (logits)\n","        logits = X.dot(W) + b\n","        # Convert logits into probabilities\n","        probs_pred = softmax(logits)\n","\n","        # Calculate loss function\n","        loss = cross_entropy(probs_true, probs_pred)\n","\n","        # Calculate gradients of loss function wrt model parameters\n","        # d(loss)/d(logits)\n","        gradients = (probs_pred - probs_true) / num_samples\n","        # d(loss)/dW\n","        dW = (X.T).dot(gradients)\n","        # d(loss)/db\n","        db = np.sum(gradients, axis=0, keepdims=True)\n","\n","        # Update model parameters (gradient descent)\n","        W -= rate * dW\n","        b -= rate * db\n","\n","        # Print out loss function values\n","        if epoch % 5 == 0: print(f\"Epoch {epoch}, loss {loss:.4f}\")\n","\n","    # Return model parameters\n","    return W, b\n","\n","# Train model\n","# Initial run:\n","W0, b0 = train(X,Y)\n","# Additional runs:\n","W0,b0 = train(X,Y,rate=0.01,num_epochs=1000,W=W0,b=b0)"],"metadata":{"id":"0tE65vW2SDba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate predicted values.\n","# Outputs are classes with largest probabilities.\n","def predict(X, W, b):\n","    logits = X.dot(W) + b\n","    return np.argmax(logits, axis=1)\n","\n","Y_pred = predict(X, W0, b0)\n","\n","accuracy = np.mean(Y_pred == Y)\n","print(\"Training accuracy:\", accuracy)"],"metadata":{"id":"ntzOkguU39PU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explanation of train:\n","\n","# Initialize matrices\n","# W = random matrix (784*10)\n","# b = bias, initially zeros (1*10)\n","num_classes = 10\n","N, D = X.shape\n","W = np.random.randn(D, num_classes) * 0.01\n","b = np.zeros((1, num_classes))\n","\n","# Convert labels into probability vectors\n","answers = one_hot(Y.astype(int),num_classes)\n","\n","# Calculate linear function from inputs\n","scores = X.dot(W)+b\n","\n","# Convert scores to probabilities\n","probs = softmax(scores)\n","\n","# Sum of probs = 1\n","#print(\"Sum of probs:\",np.sum(probs[1]))\n","\n","# Compare predicted probs with answers\n","loss = cross_entropy(answers,probs)\n","\n","print(\"Loss:\",loss)\n","\n","# Main magic: gradient descent\n","\n","# grad_scores = d(loss)/d(scores)\n","grad_scores = (probs-answers)/N\n","\n","#print(grad_scores)\n","\n","# Derivative d(loss)/dW\n","dW = (X.T).dot(grad_scores)\n","\n","# Derivative d(loss)/db\n","db = np.sum(grad_scores, axis=0, keepdims=True)"],"metadata":{"id":"31w-sioL3-4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jXfKB-z42jpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(Y_pred[:20])\n","print(Y[:20].astype(int))"],"metadata":{"id":"DqOFIY2VB5wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W = W0"],"metadata":{"id":"bPTgi0q_CED1"},"execution_count":null,"outputs":[]}]}