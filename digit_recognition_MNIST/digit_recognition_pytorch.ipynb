{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNb5DOAmhZz7MRW2U163eHX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2YX6flah-HGj"},"outputs":[],"source":["# Mount Google Drive storage\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Import training data\n","# First row = labels as strings, every other row = (answer,pixel data array)\n","data_train_total = pd.read_csv( \\\n","                \"/content/drive/MyDrive/google_colaboratory/data/train.csv\", \\\n","                header=0,dtype=float).values\n","\n","# Define total number of data samples\n","num_samples_total = data_train_total.shape[0]\n","\n","# Randomly permute indices\n","indices = torch.randperm(num_samples_total)\n","\n","# Split data into \"train\" and \"validate\" datasets\n","split = int(0.8 * num_samples_total)\n","indices_train, indices_validate = indices[:split], indices[split:]\n","data_train = data_train_total[indices_train]\n","data_validate = data_train_total[indices_validate]\n","\n","# Import test data\n","data_test = pd.read_csv( \\\n","                \"/content/drive/MyDrive/google_colaboratory/data/test.csv\", \\\n","                header=0,dtype=float).values\n"]},{"cell_type":"code","source":["# Import libraries\n","import torch\n","from typing import List, Tuple\n","\n","# Use GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device, \" - \", torch.cuda.get_device_name(0))\n","\n","# Numerical precision\n","float_type = torch.float32\n","\n","# PyTorch parameters\n","torch_params = {\"dtype\": float_type, \"device\": device}\n","\n","\n","# DEFINE MODEL'S PARAMETERS, INPUTS AND OUTPUTS\n","\n","# Inputs:\n","# X = array(num_samples,num_features)\n","# (for each sample, pixel values in range [0,1], flattened 28x28-pixel image)\n","X = torch.tensor(data_train[:,1:]/255.0, **torch_params)\n","# Mean subtraction: subtract average across dataset, to make features balanced\n","X = (X - X.mean(0)) / (X.std(0)+10**(-9))\n","\n","# Outputs:\n","# Y = array(num_samples)\n","# (for each sample, correct class = digit)\n","Y = torch.tensor(data_train[:,0], dtype=torch.long, device=device)\n","\n","# Number of classes for classification (digits -> 10 possible classes)\n","# TODO: determine number of classes from data\n","num_classes = 10\n","\n","# Number of samples (data points)\n","num_samples = X.shape[0]\n","\n","# Number of features (pixels in a single sample)\n","num_features = X.shape[1]\n","\n","# Number of hidden layers\n","num_layers = 5\n","\n","# Dimension of a hidden layer\n","num_hidden = 512\n","\n","# Generate array of initial parameters (Kaiming initialization)\n","def generate_parameter(dim_in: int, dim_out: int) -> Tuple[torch.Tensor,torch.Tensor]:\n","    W = torch.sqrt(torch.tensor(2.0/dim_in)) * torch.randn(dim_in,dim_out,**torch_params)\n","    b = torch.full((1, dim_out), 0.01, **torch_params)\n","    return (W, b)\n","layer_dims = [num_features] + [num_hidden]*(num_layers-1) + [num_classes]\n","parameters_init: Tuple[Tuple[torch.Tensor, torch.Tensor]] = tuple(\\\n","    generate_parameter(dim_in, dim_out) for dim_in, dim_out in zip(layer_dims[:-1], layer_dims[1:]))\n","#if num_layers==1:\n","#    parameters_init = [generate_parameter(num_features,num_classes)]\n","#else:\n","#    parameters_init = [generate_parameter(num_features,num_hidden)]\n","#    for layer in range(2,num_layers):\n","#        parameters_init.append(generate_parameter(num_hidden,num_hidden))\n","#    parameters_init.append(generate_parameter(num_hidden,num_classes))\n","\n","#-------------------------------------------------------------------------------\n","\n","# DEFINE FUNCTIONS\n","\n","# ReLU activation function (used for hidden layers): removes negative scores\n","def relu(Z: torch.Tensor) -> torch.Tensor:\n","    return torch.maximum(Z,torch.tensor(0.0,**torch_params))\n","def relu_derivative(Z: torch.Tensor) -> torch.Tensor: return (Z>0).float()\n","\n","# SOFTMAX activation function (used for output layer):\n","# transforms arbitrary-valued scores (logits) into probabilities\n","# Z = array(num_samples,num_classes)\n","# (for each sample, arbitrary-valued scores for each class)\n","# softmax(Z) = array(num_samples,num_classes)\n","# (for each sample, probability of belonging to a certain class)\n","def softmax(Z: torch.Tensor) -> torch.Tensor:\n","    exp_z = torch.exp(Z - Z.max(1, keepdim=True).values)\n","    return exp_z / exp_z.sum(1, keepdim=True)\n","\n","# Function that converts outputs (labels) into corresponding probability vectors\n","# Y = array(num_samples)\n","# one_hot(Y) = array(num_samples,num_classes)\n","# (for each sample, probability of belonging to a certain class)\n","def one_hot(Y: torch.Tensor) -> torch.Tensor:\n","    return torch.eye(num_classes,**torch_params)[Y]\n","\n","# LOSS FUNCTION - cross-entropy: calculates the punishment for a bad prediction\n","# = LOG(cummulative probability of predicting all outputs)\n","def cross_entropy(probs_pred: torch.Tensor, probs_true: torch.Tensor) -> torch.Tensor:\n","    # 10**(-9) is needed to avoid LOG(0) error\n","    return -(probs_true * torch.log(probs_pred + 10**(-9))).sum() / num_samples\n","\n","def cross_entropy_derivative(probs_pred: torch.Tensor, probs_true: torch.Tensor) -> torch.Tensor:\n","    return (probs_pred - probs_true) / num_samples\n","\n","# FORWARD PROPAGATION\n","\n","# Single-layer forward propagation\n","def forward_SL(X, W, b, activation_function):\n","    # Calculate arbitrary-valued scores\n","    Z = X @ W + b\n","    # Apply the activation function\n","    Y = activation_function(Z)\n","    return [Z, Y]\n","\n","# Multi-layer forward propagation\n","# Examples:\n","# params_array = [[W1,b1],[W2,b2],...]\n","def forward_ML(X_init, parameters):\n","    forward_parameters, X = [[0, X_init]], X_init\n","    # For every layer: calculate forward propagation and update initial vector\n","    for index, parameter in enumerate(parameters):\n","        # softmax is only applied at the outer level\n","        activation_function = relu if index!=(num_layers-1) else softmax\n","        forward_parameter = forward_SL(X,*parameter,activation_function)\n","        X = forward_parameter[1]\n","        forward_parameters.append(forward_parameter)\n","    # Export list of [Z, Y] for all layers:\n","    # index 0 corresponds to the initial data, so len(forward_params)=num_layers+1\n","    return forward_parameters\n","\n","# BACKWARD PROPAGATION\n","\n","# Single-layer backward propagation\n","# Examples:\n","# params = [cross_entropy_derivative, probs_true]\n","# params = [relu_derivative, W_next, dL_dZ_next]\n","def backward_SL(X, Z, derivative_parameters):\n","    function_derivative = derivative_parameters[0]\n","    # Cross-entropy is always the outer layer: probs_pred = Z\n","    if function_derivative.__name__=='cross_entropy_derivative':\n","        probs_pred, probs_true = softmax(Z), derivative_parameters[1]\n","        dL_dZ = cross_entropy_derivative(probs_pred, probs_true)\n","    # ReLU is always an inner layer, so it needs:\n","    # W_next and dL_dZ_next from the \"next\" (in terms of forward propagation) step\n","    elif function_derivative.__name__=='relu_derivative':\n","        W_next, dL_dZ_next = derivative_parameters[1:]\n","        # Current step's Y is \"next\" step's X\n","        dL_dY = dL_dZ_next @ W_next.T\n","        dL_dZ = dL_dY * relu_derivative(Z)\n","    else:\n","        raise ValueError(\"Wrong derivative function!\")\n","    # This works for every layer: Z = W*X + b\n","    dL_dW = X.T @ dL_dZ\n","    dL_db = dL_dZ.sum(0, keepdim=True)\n","    # Export derivatives\n","    return [dL_dZ, dL_dW, dL_db]\n","\n","# Multi-layer backward propagation\n","def backward_ML(X_init, probs_true, parameters, forward_parameters):\n","    derivatives = []\n","    # Layers are counted in the direction of forward propagation\n","    # layer 0 is the initial data\n","    for layer in range(num_layers-1,-1,-1):\n","        # If it is the outer layer, apply cross_entroy_derivative\n","        # If it is an inner layer, apply relu_derivative with:\n","        # W_next = parameters[layer-1,1]\n","        # dL_dZ_next = derivatives[0,0]\n","        backward_parameters = \\\n","            [cross_entropy_derivative, probs_true] if layer==num_layers-1 else \\\n","            [relu_derivative, parameters[layer+1][0], derivatives[0][0]]\n","        X, Z = forward_parameters[layer][1], forward_parameters[layer+1][0]\n","        derivative = backward_SL(X, Z, backward_parameters)\n","        derivatives.insert(0,derivative)\n","    return derivatives\n","\n","# Training function\n","def train(X, Y, parameters = None, \\\n","          rate_init=0.05, num_epochs=300, lambda_reg=0.0001, momentum=0.9):\n","\n","    # Assign default parameters\n","    #if parameters is None:\n","    #    parameters = [[W.copy(), b.copy()] for (W,b) in parameters_init]\n","    if parameters is None:\n","        parameters = [[W.clone(), b.clone()] for (W, b) in parameters_init]\n","\n","    # Probabilities of outputs\n","    probs_true = one_hot(Y)\n","\n","    # Initialize velocities\n","    # velocities = [[v_W1,v_b1],[v_W2,v_b2],...]\n","    velocities = [[torch.zeros_like(W), torch.zeros_like(b)] for (W, b) in parameters]\n","\n","    # Iterate over epochs\n","    for epoch in range(num_epochs):\n","\n","        # Forward propagation\n","        forward_parameters = forward_ML(X, parameters)\n","\n","        # Backward propagation\n","        derivatives = backward_ML(X, probs_true, parameters, forward_parameters)\n","\n","        # Iterate over layers\n","        for layer in range(num_layers):\n","            # L2-regularization\n","            derivatives[layer][1] += lambda_reg*parameters[layer][0]\n","            # Re-calculate velocities\n","            velocities[layer][0] = momentum*velocities[layer][0] - rate_init*derivatives[layer][1]\n","            velocities[layer][1] = momentum*velocities[layer][1] - rate_init*derivatives[layer][2]\n","            # Perform gradient descent\n","            parameters[layer][0] += velocities[layer][0]\n","            parameters[layer][1] += velocities[layer][1]\n","\n","        # Print out loss function values\n","        if epoch % 10 == 0:\n","            # Calculate loss\n","            loss = cross_entropy(forward_parameters[-1][1],probs_true)\n","            # Calculate accuracy\n","            accuracy = (forward_parameters[-1][1].argmax(1) == Y).float().mean().item()\n","            print(f\"Epoch {epoch}, loss {loss:.4f}, accuracy {accuracy:.4f}\")\n","\n","    return parameters"],"metadata":{"id":"NpLwRC48-ROP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters_train = train(X, Y, num_epochs=500)"],"metadata":{"id":"6-USzZbxXAWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inputs:\n","# X = array(num_samples,num_features)\n","# (for each sample, pixel values in range [0,1], flattened 28x28-pixel image)\n","X_validate = torch.tensor(data_validate[:,1:]/255.0, **torch_params)\n","# Mean subtraction: subtract average across dataset, to make features balanced\n","#X_validate = (X_validate - X_validate.mean(0)) / (X_validate.std(0)+10**(-9))\n","\n","# Outputs:\n","# Y = array(num_samples)\n","# (for each sample, correct class = digit)\n","Y_validate = torch.tensor(data_validate[:,0], dtype=torch.long, device=device)\n","\n","# Check the validate accuracy\n","#probs_validate = forward_ML(X_validate,parameters_train)[-1][0]\n","\n","def validate(X_validate: torch.Tensor, Y_validate: torch.Tensor, parameters: list):\n","    # One-hot labels for validation\n","    probs_true = one_hot(Y_validate)\n","\n","    # Forward pass\n","    forward_parameters = forward_ML(X_validate, parameters)\n","    probs_pred = forward_parameters[-1][1]\n","\n","    # Loss\n","    loss = cross_entropy(probs_pred, probs_true)\n","\n","    # Accuracy\n","    accuracy = (probs_pred.argmax(1) == Y_validate).float().mean().item()\n","\n","    print(f\"Validation loss {loss:.4f}, accuracy {accuracy:.4f}\")\n","    return loss.item(), accuracy\n","\n","val_loss, val_acc = validate(X_validate, Y_validate, parameters_train)"],"metadata":{"id":"gS1jxi__Y6Ak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["probs_validate[0]"],"metadata":{"id":"Dwr2PAM5ZxQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate predicted values.\n","# Outputs are classes with largest probabilities.\n","def predict(X, W1, b1, W2, b2):\n","    Z1 = X.dot(W1) + b1\n","    H = relu(Z1)\n","    Z2 = H.dot(W2) + b2\n","    probs_pred = softmax(Z2)\n","    return np.argmax(probs_pred, axis=1)\n","\n","Y_pred = predict(X, W1_0, b1_0, W2_0, b2_0)\n","\n","accuracy = np.mean(Y_pred == Y)\n","print(\"Training accuracy:\", accuracy)"],"metadata":{"id":"eBXB2rl7Ysdj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define tiny example\n","W = torch.tensor([[1.0, 2.0, 3.0],\n","                  [4.0, 5.0, 6.0]])   # shape (2, 3)\n","\n","x = torch.tensor([1.0, 1.0, 1.0])      # shape (3,)\n","b = torch.tensor([0.5, -0.5])          # shape (2,)\n","\n","print(\"W:\\n\", W)\n","print(\"x:\\n\", x)\n","print(\"b:\\n\", b)\n","\n","# --- Method 1: using @ operator (transpose W to match shapes)\n","y1 = W @ x + b\n","\n","# --- Method 2: using torch.matmul (same thing)\n","y2 = torch.matmul(W, x) + b\n","\n","# --- Method 3: using torch.addmm (expects 2D input, so reshape x)\n","y3 = torch.addmm(b, x.unsqueeze(0), W.T)  # x as (1,3), W.T as (3,2)\n","\n","print(\"\\nMethod 1 ( @ ):\\n\", y1)\n","print(\"\\nMethod 2 (matmul):\\n\", y2)\n","print(\"\\nMethod 3 (addmm):\\n\", y3)"],"metadata":{"id":"XLJCUJYaFy0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"suEqY32pFuQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4sotkdFEC1Rp"},"execution_count":null,"outputs":[]}]}